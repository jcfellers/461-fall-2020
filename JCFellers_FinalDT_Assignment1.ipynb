{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JCFellers_FinalDT_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBV6xXsOnwjjMVPoCzN3wm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcfellers/461-fall-2020/blob/master/JCFellers_FinalDT_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z-FRhVfJHgs"
      },
      "source": [
        "Justin Fellers - Assignment 1 Abstract \n",
        "\n",
        "I've been programming in python for about 4 months now, starting just at the end last semester.  This is my first significant implementation using a Class and a nested dictionary. This exercise really stretched my mind and I learned so much.  In addition to your lecture notes and the video you posted, I also consulted the following references, with number 4 being the one I got the most out of to construct my code in a similar fashion:\n",
        "\n",
        "1) https://blog.clairvoyantsoft.com/entropy-information-gain-and-gini-index-the-crux-of-a-decision-tree-99d0cdc699f4\n",
        "\n",
        "2) https://scikit-learn.org/0.15/modules/generated/sklearn.cross_validation.train_test_split.html\n",
        "\n",
        "3) https://medium.com/@dhiraj8899/decision-tree-from-scratch-in-python-629631ec3e3a\n",
        "\n",
        "4) https://medium.com/swlh/decision-tree-implementation-from-scratch-in-python-1cff4c00c71f\n",
        "\n",
        "Up until now all of my coding has been in python IDLE and Spyder, which is where I coded this and why it is in a single cell.  I can send you my .py if you prefer and I will learn Jupyter/Colab before the next assignment.\n",
        "\n",
        "The outline of the code is as follows.  The DTClassifer object is created with a user-parameter for max_depth of the tree.  The when fitted with an input X and input Y, the object concatenates the two into 'df' for manipulation.  The object then goes into building the tree with the '_build_tree(df)' operation. Then the following steps/substeps occur:\n",
        "\n",
        "  1) produce the feature to serve as the node and the threshold to separate continuous features by using '_choose_feature_split(df)': \n",
        "  \n",
        "  1a) calculate the df entropy by using '_calc_parent_entropy(df)'. The df's entropy value is returned. \n",
        "\n",
        "  1b) calculate the entropies of all features by using '_calc_feature_entropy(df, feature)'. The entropy and threshold of each feature are returned. \n",
        "  \n",
        "  1b1) if the feature is categorical, the entropy of each feature value is computed then muliplied by the weight factor (feature value count/total df count).  These are the summands of the feature's total entropy. \n",
        "\n",
        "  1b2) if the feature is numerical, the feature is split at a threshold value assigned as the median value of the df's feature.  The left child is for features <= the threshold, and the right child is for the features > the threshold.  The entropies of the two children are the summands for the feature's total entropy.  \n",
        "\n",
        "  1c) the '_choose_feature_split(df)' calculates the information gain for each feature by subracting its entropy from that of the parent entropy. The feature which produces the highest information gain and the threshold are returned to  '_build_tree(df)'\n",
        "\n",
        "  2a) With feature/threshold returned, if the feature is categorical then a subset of records from df is created by '_split_categorical_feature(df, feature, v)'. If this subset only has 1 of the target values in it, this indicates a homogenous target set and the target value it is added to the tree as a leaf value with the feature value serving as the node.  Otherwise, this subset is recursed back into '_build_tree(df)' as the df.  A counter keeps track of the depth, and if it is equal to the max depth parameter the target which appears most in the subset is assigned as the leaf.  \n",
        "\n",
        "  2b) If the feature is numeric, two subsets are created: one for the left child where the feature is <= threshold and the other for the right child where the feature is > threshold.  For the left subset and for the right subset, both are evaluated for homogenous target values to be append as a leaf or recursively enter '_build_tree(df)' the same as 2a.  \n",
        "\n",
        "  3a) Once the tree is built, predictions for each row in the input X are made. Each row and the constructed tree are input to '_predict_target_value(row, tree)'. \n",
        "\n",
        "  3b) For each node of the tree, the imported row is assigned to a variable that represents its branch assignment.  This branch is then read back into the function to start over with the next node, until the branch reaches its leaf value.  The eventual leaf value is the predicted value for the row.  \n",
        "\n",
        "  4) After all the predictions have been made, they are stored in the Series 'prediction_results'.  \n",
        "\n",
        "  5) To calculate the accuracy, the number of times a row's actual target is equal to the predicted target is counted and divided by the number of actual targets. \n",
        "\n",
        "\n",
        "Below are further explanations about using the algorithm on the respective datasets.  Thank you for your time in reviewing my work and I look forward to hearing your feedback.  I'm a novice to programming and ML but want to keep learning.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78ESZHg6yNub"
      },
      "source": [
        "------------------------------------------------------------------------------\n",
        "Titanic Dataset Discussion\n",
        "\n",
        "\n",
        "The code starts by importing the dataset and seeing if there is any column where each record is a unique value.  If so, it drops the column.  This prevents the algorithm from memorizing the dataset and overfitting.  For this dataset, the 'Names' column is dropped.  For the titanic dataset I convert the 'Pclass' datatype into a string so it may be considered as a category.  One issue I ran into with this algorithm is that certain subsets never distill down to a single target.  I noticed this with the 'Pclass' in particular.  For example, say 'Age' is chosen first as the feature to split on and the threshold is 28.  Then 'Pclass' is chosen next.  There is a liklihood both target values will be the subset without another other feature to further split on, such as when the 'Fare' is 0 the family categories are also 0.  This leads the DT to run indefinitely, which I know from lecture is not supposed to occur.  I rectified this issue with a bypass where if the same subet of rows occur back to back, the target which is most frequent in the subset is chosen as the leaf value.  This  prevents the tree from running indefinitely if given a max_depth of None and works decently well for its intended purpose, as shown by an accuracy score of 98% on training data when allowed to run without a maximum depth.   \n",
        "\n",
        "The results of this tree on this dataset are interesting, as it appears the tree is just as effective at making predictions with a depth of 1 as it is with any value of increased depth.  The feature 'Fare' is nearly always selected as the first split, and when conducting an analysis to determine the best depth the results for the train and test set display minimal variance.  I believe this is connected to the tree's inability to distill down to every record given the design I have implemented.  Experimenting with the size of the tree depth also highlighted an error with the dictionary method when predicting the test set.  On occasion, there is a node in the tree that does not split on a feature value that is in a test record.  For example, the fitted tree may split first on 'Fare', with children nodes under a threshold and above a threshold value.  Then say the right child splits on a 'Pclass' value of 3 and the left child splits on a 'Pclass' value of 1.  If the test record has a value of 2, then the dictionary returns a KeyError.  I could not implement a fix to this issue, although noticed it rarely due to the high frequency at which other Features are split on before 'Pclass' is used.  To prevent this error from crashing the script, I instituted an exception with a returned prediciton of randomly choosing one of the target values.  \n",
        "\n",
        "As can be seen in the print window below, I experimented with four different depth sizes: 10, 20, 30, 40.  I'm not sure if this an extensive enough range, but I think it is for a dataset with such few features.  For each one, a 80/20 split of train/test data is established and a tree is fitted with the train data.  Then the tree is tested for accuracy on the train set and the test set.  As can be seen, neither the accuracy for the training nor the testing sets vary.  What I should have seen here is an increase in training accuracy with a decrease in testing accuracy as the got larger.  Also shown are accuracy scores for training and testing without a maximum depth.  Note the high score on the training set to indicate overfitting, and although the accuracy score on the test set is higher I know the tree is overfit.  Whenever the KeyError bypass is implemented to avoid crashing the line prints, so in this case the last time I ran the script it occur one time in fitting the 5 trees of various depths and twice when fitting the tree with no maximum depth.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNlCAtALDsvp",
        "outputId": "39565c5a-c190-4072-9b45-164497a71ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from statistics import mean\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "eps = np.finfo(float).eps\n",
        "\n",
        "# decision tree classifer object\n",
        "class DTClassifier:\n",
        "    \n",
        "    def __init__(self, max_depth = 50):\n",
        "        self.depth = 0\n",
        "        self.max_depth = max_depth      \n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        df = X.copy()\n",
        "        df['Target'] = y.copy()\n",
        "        self.tree = self._build_tree(df)\n",
        "        \n",
        "        #print ('\\nDT(depth = %s : \\n %s' % (self.depth, self.tree))\n",
        "\n",
        "    def _build_tree(self, df, tree = None, prev_df = None):\n",
        "        \n",
        "        feature, threshold = self._choose_feature_split(df)\n",
        "        #print(feature, threshold)\n",
        "        \n",
        "        if tree is None:\n",
        "            tree = {}\n",
        "            tree[feature] = {}\n",
        "                    \n",
        "        # Categorical Feature Build\n",
        "        if threshold == None:\n",
        "            datum = np.unique(df[feature])\n",
        "            \n",
        "            for v in datum:\n",
        "                v_df = self._split_categorical_feature(df, feature, v)\n",
        "                v_df_t, v_df_tc = np.unique(v_df['Target'], return_counts = True)\n",
        "                \n",
        "                if v_df.equals(prev_df):\n",
        "                    tree[feature][v] = v_df_t[np.argmax(v_df_tc)]\n",
        "                    #print('no improvement, assigned target')\n",
        "                    break\n",
        "                \n",
        "                v2_df = v_df.copy()\n",
        "                \n",
        "                \n",
        "                if len(v_df_t) == 1: \n",
        "                    tree[feature][v] = v_df_t[0]\n",
        "                else:\n",
        "                  self.depth += 1\n",
        "                  \n",
        "                  if self.max_depth is not None and self.depth > self.max_depth:\n",
        "                      tree[feature][v] = v_df_t[np.argmax(v_df_tc)]\n",
        "                      \n",
        "                  else:\n",
        "                      tree[feature][v] = self._build_tree(v_df, prev_df = v2_df)\n",
        "                      \n",
        "        # Numerical Feature Build - LC for <=, RC for > \n",
        "        else:\n",
        "            lc_df, rc_df = self._split_numerical_feature(df, feature, threshold)\n",
        "            \n",
        "            lc_df_t, lc_df_tc = np.unique(lc_df['Target'], return_counts = True)\n",
        "            rc_df_t, rc_df_tc = np.unique(rc_df['Target'], return_counts = True)\n",
        "            #print(lc_df['Target'])\n",
        "            if len(lc_df_t) == 1:\n",
        "                tree[feature]['<=' + str(threshold)] = lc_df_t[0]\n",
        "            else:\n",
        "                self.depth += 1\n",
        "                if self.max_depth is not None and self.depth > self.max_depth:\n",
        "                    tree[feature]['<=' + str(threshold)] = lc_df_t[np.argmax(lc_df_tc)]\n",
        "                else:\n",
        "                    tree[feature]['<=' + str(threshold)] = self._build_tree(lc_df)\n",
        "            \n",
        "            if len(rc_df_t) == 1:\n",
        "                tree[feature]['>' + str(threshold)] = rc_df_t[0]\n",
        "            else:\n",
        "                self.depth += 1\n",
        "                if self.max_depth is not None and self.depth > self.max_depth:\n",
        "                    tree[feature]['>' + str(threshold)] = rc_df_t[np.argmax(rc_df_tc)]\n",
        "                else:\n",
        "                    tree[feature]['>' + str(threshold)] = self._build_tree(rc_df)\n",
        "           \n",
        "            \n",
        "        #print(tree.keys())\n",
        "        return tree\n",
        "                \n",
        "##############################################################################\n",
        "\n",
        "    def _split_categorical_feature(self, df, feature, feature_value):\n",
        "        \n",
        "        v_df = df[df[feature] == feature_value]\n",
        "        #print(v_df)\n",
        "        return v_df\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "    def _split_numerical_feature(self, df, feature, threshold):\n",
        "        \n",
        "        lc_df = df[df[feature] <= threshold]\n",
        "        rc_df = df[df[feature] > threshold]\n",
        "        #print(lc_df, rc_df)\n",
        "        return lc_df, rc_df\n",
        "            \n",
        "        \n",
        "##############################################################################\n",
        "        \n",
        "        \n",
        "    def _choose_feature_split(self, df):\n",
        "        # lists for information gain values and thresholds\n",
        "        info_gains = []\n",
        "        thresholds = []\n",
        "        featureLst = X.columns.to_list()\n",
        "        for f in featureLst:\n",
        "            parent_entropy = self._calc_parent_entropy(df) \n",
        "            feature_entropy, threshold = self._calc_feature_entropy(df, f)\n",
        "            IG = parent_entropy - feature_entropy\n",
        "            \n",
        "            info_gains.append(IG)\n",
        "            thresholds.append(threshold)\n",
        "        \n",
        "        feature = featureLst[np.argmax(info_gains)]\n",
        "        threshold = thresholds[np.argmax(info_gains)]\n",
        "                \n",
        "        return feature, threshold\n",
        "            \n",
        "\n",
        "###############################################################################\n",
        "\n",
        "    def _calc_parent_entropy(self, df):\n",
        "        parent_entropy = 0\n",
        "        datum = np.unique(df['Target'], return_counts = True)\n",
        "        total = sum(datum[1])\n",
        "                        \n",
        "        for value in datum[0]:\n",
        "            parent_entropy -= (datum[1][value]/total) * np.log2(datum[1][value]/total)\n",
        "            \n",
        "        return parent_entropy\n",
        "                \n",
        "    \n",
        "##############################################################################\n",
        "    \n",
        "    def _calc_feature_entropy(self, df, feature):\n",
        "        #feature = 'Pclass'\n",
        "        feature_entropy = 0\n",
        "        threshold = None\n",
        "        \n",
        "        # Categorical features\n",
        "        if (df[feature].dtypes == object):\n",
        "            datum = np.unique(df[feature])\n",
        "            \n",
        "            for v in datum:\n",
        "                v_entropy = 0                \n",
        "                v_df = df[df[feature] == v]\n",
        "                v_count = len(v_df)\n",
        "                #print(v_count)\n",
        "                \n",
        "                for t in np.unique(df['Target']):\n",
        "                    t_df = df[df['Target'] == t]\n",
        "                    v_t_count = len(t_df[t_df[feature] == v])\n",
        "                    #print(v_t_count)\n",
        "                    fraction = v_t_count / (v_count + eps)\n",
        "                    if (fraction > 0):\n",
        "                        v_entropy -= fraction * np.log2(fraction)\n",
        "                        \n",
        "                w = v_count/len(df)\n",
        "                feature_entropy += w * v_entropy\n",
        "            \n",
        "            #print(feature_entropy)\n",
        "        \n",
        "        # Numerical features - split into >= median value as LC, < as RC\n",
        "        elif (df[feature].dtypes != object):\n",
        "            threshold = df[feature].median()\n",
        "            lc_entropy = 0\n",
        "            rc_entropy = 0            \n",
        "            \n",
        "            lc_df = df[df[feature] <= threshold]\n",
        "            rc_df = df[df[feature] > threshold]\n",
        "            lc_count = len(lc_df)\n",
        "            rc_count = len(rc_df)\n",
        "\n",
        "            for t in np.unique(df['Target']): \n",
        "                lc_t_count = len(lc_df[lc_df['Target'] == t])\n",
        "                rc_t_count = len(rc_df[rc_df['Target'] == t])\n",
        "                fraction_lc = lc_t_count/(lc_count + eps)\n",
        "                fraction_rc = rc_t_count/(rc_count + eps)\n",
        "                \n",
        "                if (fraction_lc > 0):\n",
        "                    lc_entropy -= fraction_lc * np.log2(fraction_lc)\n",
        "                elif (fraction_rc > 0):\n",
        "                    rc_entropy -= fraction_rc * np.log2(fraction_rc)\n",
        "                      \n",
        "            w_lc = lc_count/len(df)\n",
        "            w_rc = rc_count/len(df)\n",
        "            lc_entropy = w_lc * lc_entropy\n",
        "            rc_entropy = w_rc * rc_entropy\n",
        "            \n",
        "            if (lc_entropy + rc_entropy) < 1:\n",
        "                feature_entropy = lc_entropy + rc_entropy\n",
        "            else:\n",
        "                feature_entropy = 1\n",
        "                \n",
        "            #print(feature_entropy)\n",
        "                \n",
        "        return feature_entropy, threshold\n",
        "\n",
        "##############################################################################\n",
        "    \n",
        "    def predict(self, X):\n",
        "        results = []\n",
        "        #feature_search = {}\n",
        "        #print(X.index)\n",
        "        #col_index = 0\n",
        "        # for column in X.columns:\n",
        "        #     feature_search[column] = col_index\n",
        "        #     col_index += 1\n",
        "\n",
        "        for i in X.index:\n",
        "            #print (i)\n",
        "            row_prediction = self._predict_target_value(X.loc[i], self.tree)\n",
        "            results.append([i, row_prediction])\n",
        "        \n",
        "        results = pd.Series(results)\n",
        "        #print(results)\n",
        "        return results\n",
        "        \n",
        "                    \n",
        "###############################################################################\n",
        "    \n",
        "    def _predict_target_value(self, row, tree):\n",
        "        try:\n",
        "\n",
        "            for node in tree.keys():\n",
        "                result = row[node]\n",
        "                \n",
        "                if type(result) == str:\n",
        "                    #print(result)\n",
        "                    leaf = tree[node][result]\n",
        "                else:\n",
        "                    threshold = str(list(tree[node].keys())[0]).split('<=')[1]\n",
        "                    #print('t is', threshold)\n",
        "                    if result <= float(threshold):\n",
        "                        leaf = tree[node]['<=' + str(threshold)]\n",
        "                        \n",
        "                    elif result > float(threshold):\n",
        "                        leaf = tree[node]['>' + str(threshold)]\n",
        "                        \n",
        "                if type(leaf) is dict:\n",
        "                    prediction = self._predict_target_value(row, leaf)\n",
        "                    \n",
        "                else:\n",
        "                    prediction = leaf\n",
        "                    \n",
        "            return prediction\n",
        "\n",
        "        except(KeyError):\n",
        "          print('keyerror encountered, returned random target value')\n",
        "          prediction = random.choice([0, 1])\n",
        "          return prediction\n",
        "###############################################################################\n",
        "def calc_accuracy(y_actual, y_pred):\n",
        "    #print(y_actual)\n",
        "    \n",
        "    #acc = number of correct predictions / number of predictions\n",
        "    \n",
        "    count_correct = 0\n",
        "    for i in range(len(y_actual)):\n",
        "        actual_target = y_actual.iloc[i]\n",
        "        predicted_target = y_pred.iloc[i][1]\n",
        "        \n",
        "        if predicted_target == actual_target:\n",
        "                count_correct += 1\n",
        "\n",
        "    \n",
        "    acc = round((count_correct/len(y_actual) * 100),2)\n",
        "    \n",
        "    return acc\n",
        "    \n",
        "############################################################################### \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    titanic_url = 'https://raw.githubusercontent.com/jcfellers/461-fall-2020/master/assignments/assignment-1/titanic.csv'\n",
        "\n",
        "    data = pd.read_csv(titanic_url)\n",
        "    \n",
        "    # should any column be full of only unique values, drop it\n",
        "    header = data.columns.values.tolist()\n",
        "\n",
        "    for column in header:\n",
        "      if len(data[column].unique()) == len(data):\n",
        "        data = data.drop([column], axis = 1)\n",
        "    \n",
        "    # convert Pclass to object so it is categorical\n",
        "    data['Pclass'] = data['Pclass'].astype(str)\n",
        "\n",
        "    #print(data.head())\n",
        "\n",
        "   \t#Split Features and target\n",
        "    X, y = data.drop(['Survived'], axis=1), data['Survived']\n",
        "    #print(X.shape, y.shape)\n",
        "    \n",
        "    ###########################################################################\n",
        "       \n",
        "   # Search for best depth\n",
        "    \n",
        "    trainAcc = []\n",
        "    testAcc = []\n",
        "\n",
        "    for d in range(1, 50, 10):\n",
        "        d_train_res = []\n",
        "        d_test_res = []\n",
        "        \n",
        "        for t in range(5):\n",
        "        \n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
        "        \n",
        "            dt_clf = DTClassifier(max_depth = d)\n",
        "        \n",
        "            dt_clf.fit(X_train, y_train)\n",
        "        \n",
        "                  ##############################################\n",
        "        \n",
        "            prediction_results = dt_clf.predict(X_train)\n",
        "        \n",
        "            accuracy = calc_accuracy((y_train), prediction_results)\n",
        "            d_train_res.append(accuracy)\n",
        "        \n",
        "            #print('\\n d=%s, Train Accuracy: %s' % (d, accuracy))\n",
        "        \n",
        "            prediction_results = dt_clf.predict(X_test)\n",
        "        \n",
        "            accuracy = calc_accuracy((y_test), prediction_results)\n",
        "            d_test_res.append(accuracy)\n",
        "        \n",
        "            #print('Test Accuracy: %s' % (accuracy))\n",
        "            \n",
        "        avg_d_train = round(mean(d_train_res), 2)\n",
        "        avg_d_test = round(mean(d_test_res), 2)\n",
        "        \n",
        "        trainAcc.append(avg_d_train)\n",
        "        testAcc.append(avg_d_test)\n",
        "    \n",
        "    print('Average training accuracy from 5 iterations by increasing depth left to right\\n', trainAcc)\n",
        "    print('Average testing accuracy from 5 iterations by increasing depth left to right\\n', testAcc)\n",
        "\n",
        "  ##############################################    \n",
        "    print ('\\n')\n",
        "    print ('Train set and Test set accuracy when tree is given no maximum depth.')\n",
        "\n",
        "    d = None\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
        "\n",
        "    dt_clf = DTClassifier(max_depth = d)\n",
        "\n",
        "    dt_clf.fit(X_train, y_train)\n",
        "\n",
        "    prediction_results = dt_clf.predict(X_train)\n",
        "\n",
        "    accuracy = calc_accuracy((y_train), prediction_results)\n",
        "    \n",
        "    print('\\n d=%s, Train Accuracy: %s' % (d, accuracy))\n",
        "\n",
        "    prediction_results = dt_clf.predict(X_test)\n",
        "\n",
        "    accuracy = calc_accuracy((y_test), prediction_results)\n",
        "\n",
        "    print('\\n d=%s, Test Accuracy: %s' % (d, accuracy))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keyerror encountered, returned random target value\n",
            "Average training accuracy from 5 iterations by increasing depth left to right\n",
            " [64.12, 63.1, 64.57, 65.28, 66.46]\n",
            "Average testing accuracy from 5 iterations by increasing depth left to right\n",
            " [61.68, 64.61, 65.06, 62.47, 62.81]\n",
            "\n",
            "\n",
            "Train set and Test set accuracy when tree is given no maximum depth.\n",
            "\n",
            " d=None, Train Accuracy: 97.88\n",
            "keyerror encountered, returned random target value\n",
            "keyerror encountered, returned random target value\n",
            "\n",
            " d=None, Test Accuracy: 75.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPgSs3aXA1U2"
      },
      "source": [
        "_______________________________________________________________________________\n",
        "Cars Dataset Discussion\n",
        "\n",
        "The cars dataset challenged the algorithm design and further showed the difficulties presented with different datatypes.  As discussed above with the titanic dataset, the decision tree had complications with the categorical data types, but was mostly alleviated by the tree's ability to continually parse down the numerical types with different threshold values.  \n",
        "\n",
        "In the cars dataset, I first added the feature names to the dataset prior to importing it.  Then I changed all of the features to integer types between 0 and 3 for those with four feature values, and 0 and 2 for those with three feature values.  This allowed me to take advantage of the numerical threshold process that proved more effective with the titanic dataset, but presented a new issue because the feature values, while numbers, do not represent continuous features.  I had to engineer workarounds to allow node creations for branches that led no where in order to get the script to run.  In larger trees this led to more frequent KeyErrors than in the titanic dataset when predicting test data.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3A6rAfqBtPx",
        "outputId": "b4313bea-d8fb-4cdd-fb33-3d15f09de7e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdB8CzZ2B8-5",
        "outputId": "030ba458-7473-4542-eed8-2612e8aa6a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        " data = pd.read_csv('gdrive/My Drive/car.data')\n",
        " data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>buying</th>\n",
              "      <th>maint</th>\n",
              "      <th>doors</th>\n",
              "      <th>persons</th>\n",
              "      <th>lug_boot</th>\n",
              "      <th>safety</th>\n",
              "      <th>car</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>small</td>\n",
              "      <td>low</td>\n",
              "      <td>unacc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>small</td>\n",
              "      <td>med</td>\n",
              "      <td>unacc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>small</td>\n",
              "      <td>high</td>\n",
              "      <td>unacc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>med</td>\n",
              "      <td>low</td>\n",
              "      <td>unacc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vhigh</td>\n",
              "      <td>vhigh</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>med</td>\n",
              "      <td>med</td>\n",
              "      <td>unacc</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  buying  maint doors persons lug_boot safety    car\n",
              "0  vhigh  vhigh     2       2    small    low  unacc\n",
              "1  vhigh  vhigh     2       2    small    med  unacc\n",
              "2  vhigh  vhigh     2       2    small   high  unacc\n",
              "3  vhigh  vhigh     2       2      med    low  unacc\n",
              "4  vhigh  vhigh     2       2      med    med  unacc"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sG1dE81EW3a",
        "outputId": "f02270e6-e5d9-4d1a-de93-d810610f42b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statistics import mean\n",
        "\n",
        "eps = np.finfo(float).eps\n",
        "\n",
        "# decision tree classifer object\n",
        "class DTClassifier:\n",
        "    \n",
        "    def __init__(self, max_depth = None):\n",
        "        \n",
        "        self.depth = 0\n",
        "        self.max_depth = max_depth\n",
        "        self.features = list\n",
        "        self.X_train = np.array        \n",
        "        self.y_train = np.array        \n",
        "        self.num_features = int        \n",
        "        self.train_size = int\n",
        "        \n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        self.features = list(X.columns)\n",
        "        self.train_size = X.shape[0]\n",
        "        self.num_features = X.shape[1]\n",
        "        \n",
        "        # df is the X and y together, features and target, for analyses\n",
        "        df = X.copy()\n",
        "        df['Target'] = y.copy()\n",
        "                \n",
        "        # Step 1: object is constructed from fit function; begins building tree\n",
        "        self.tree = self._build_tree(df)\n",
        "        \n",
        "        #print ('\\nDT(depth = %s : \\n %s' % (self.depth, self.tree))\n",
        "\n",
        "\n",
        "    def _build_tree(self, df, tree = None, prev_df = None):\n",
        "        \n",
        "        feature, threshold = self._choose_feature_split(df)\n",
        "        #print(feature, threshold)\n",
        "        \n",
        "        if tree is None:\n",
        "            tree = {}\n",
        "            tree[feature] = {}\n",
        "                    \n",
        "        # Categorical Feature Build\n",
        "        if threshold == None:\n",
        "            datum = np.unique(df[feature])\n",
        "            \n",
        "            for v in datum:\n",
        "                v_df = self._split_categorical_feature(df, feature, v)               \n",
        "                v_df_t, v_df_tc = np.unique(v_df['Target'], return_counts = True)\n",
        "                \n",
        "                if v_df.equals(prev_df):\n",
        "                    tree[feature][v] = v_df_t[np.argmax(v_df_tc)]\n",
        "                    break\n",
        "                \n",
        "                v2_df = v_df.copy()\n",
        "                \n",
        "                if len(v_df_t) == 1: \n",
        "                    tree[feature][v] = v_df_t[0]\n",
        "                else:\n",
        "                  self.depth += 1\n",
        "                  \n",
        "                  if self.max_depth is not None and self.depth > self.max_depth:\n",
        "                      tree[feature][v] = v_df_t[np.argmax(v_df_tc)]\n",
        "                      \n",
        "                  else:\n",
        "                      tree[feature][v] = self._build_tree(v_df, prev_df = v2_df)\n",
        "                      \n",
        "        # Numerical Feature Build - LC for <=, RC for > \n",
        "        else:\n",
        "            lc_df, rc_df = self._split_numerical_feature(df, feature, threshold)\n",
        "            \n",
        "            lc_df_t, lc_df_tc = np.unique(lc_df['Target'], return_counts = True)\n",
        "            rc_df_t, rc_df_tc = np.unique(rc_df['Target'], return_counts = True)\n",
        "            \n",
        "            if len(lc_df_t) == 0:\n",
        "                tree[feature]['<=' + str(threshold)] = None\n",
        "            if len(rc_df_t) == 0:\n",
        "                tree[feature]['>' + str(threshold)] = None\n",
        "                \n",
        "            else:\n",
        "                #print(lc_df['Target'])\n",
        "                if len(lc_df_t) == 1:\n",
        "                    tree[feature]['<=' + str(threshold)] = lc_df_t[0]\n",
        "                else:\n",
        "                    self.depth += 1\n",
        "                    if self.max_depth is not None and self.depth > self.max_depth:\n",
        "                        tree[feature]['<=' + str(threshold)] = lc_df_t[np.argmax(lc_df_tc)]\n",
        "                    else:\n",
        "                        tree[feature]['<=' + str(threshold)] = self._build_tree(lc_df)\n",
        "                \n",
        "                if len(rc_df_t) == 1:\n",
        "                    tree[feature]['>' + str(threshold)] = rc_df_t[0]\n",
        "                else:\n",
        "                    self.depth += 1\n",
        "                    if self.max_depth is not None and self.depth > self.max_depth:\n",
        "                        tree[feature]['>' + str(threshold)] = rc_df_t[np.argmax(rc_df_tc)]\n",
        "                    else:\n",
        "                        tree[feature]['>' + str(threshold)] = self._build_tree(rc_df)\n",
        "           \n",
        "            \n",
        "        #print(tree.keys())\n",
        "        return tree\n",
        "                \n",
        "##############################################################################\n",
        "\n",
        "    def _split_categorical_feature(self, df, feature, feature_value):\n",
        "        \n",
        "        v_df = df[df[feature] == feature_value]\n",
        "        #print('vDF', v_df)\n",
        "        return v_df\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "    def _split_numerical_feature(self, df, feature, threshold):\n",
        "        \n",
        "        lc_df = df[df[feature] <= threshold]\n",
        "        rc_df = df[df[feature] > threshold]\n",
        "        #print(lc_df, rc_df)\n",
        "        return lc_df, rc_df\n",
        "            \n",
        "        \n",
        "##############################################################################\n",
        "        \n",
        "        \n",
        "    def _choose_feature_split(self, df):\n",
        "        # lists for information gain values and thresholds\n",
        "        info_gains = []\n",
        "        thresholds = []\n",
        "        featureLst = X.columns.to_list()\n",
        "        for f in featureLst:\n",
        "            parent_entropy = self._calc_parent_entropy(df) \n",
        "            feature_entropy, threshold = self._calc_feature_entropy(df, f)\n",
        "            IG = parent_entropy - feature_entropy\n",
        "            \n",
        "            info_gains.append(IG)\n",
        "            thresholds.append(threshold)\n",
        "        \n",
        "        feature = featureLst[np.argmax(info_gains)]\n",
        "        threshold = thresholds[np.argmax(info_gains)]\n",
        "                \n",
        "        return feature, threshold\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "    def _calc_parent_entropy(self, df):\n",
        "        parent_entropy = 0\n",
        "        datum = np.unique(df['Target'], return_counts = True)\n",
        "        total = sum(datum[1])\n",
        "        #print(datum)\n",
        "                        \n",
        "        # for value in datum[0]:\n",
        "        #     parent_entropy -= (datum[1][value]/total) * np.log2(datum[1][value]/total)\n",
        "        \n",
        "        for i in range(len(datum[0])):\n",
        "            parent_entropy -= (datum[1][i]/total) * np.log2(datum[1][i]/total)\n",
        "        return parent_entropy\n",
        "                \n",
        "    \n",
        "##############################################################################\n",
        "    \n",
        "    def _calc_feature_entropy(self, df, feature):\n",
        "        #feature = 'Pclass'\n",
        "        feature_entropy = 0\n",
        "        threshold = None\n",
        "        \n",
        "        # Categorical features\n",
        "        if (df[feature].dtypes == object):\n",
        "            datum = np.unique(df[feature])\n",
        "            \n",
        "            for v in datum:\n",
        "                v_entropy = 0                \n",
        "                v_df = df[df[feature] == v]\n",
        "                v_count = len(v_df)\n",
        "                #print(v_count)\n",
        "                \n",
        "                for t in np.unique(df['Target']):\n",
        "                    t_df = df[df['Target'] == t]\n",
        "                    v_t_count = len(t_df[t_df[feature] == v])\n",
        "                    #print(v_t_count)\n",
        "                    fraction = v_t_count / (v_count + eps)\n",
        "                    if (fraction > 0):\n",
        "                        v_entropy -= fraction * np.log2(fraction)\n",
        "                        \n",
        "                w = v_count/len(df)\n",
        "                feature_entropy += w * v_entropy\n",
        "            \n",
        "            #print(feature_entropy)\n",
        "        \n",
        "        # Numerical features - split into >= median value as LC, < as RC\n",
        "        elif (df[feature].dtypes != object):\n",
        "            threshold = df[feature].median()\n",
        "            lc_entropy = 0\n",
        "            rc_entropy = 0            \n",
        "            \n",
        "            lc_df = df[df[feature] <= threshold]\n",
        "            rc_df = df[df[feature] > threshold]\n",
        "            lc_count = len(lc_df)\n",
        "            rc_count = len(rc_df)\n",
        "\n",
        "            for t in np.unique(df['Target']): \n",
        "                lc_t_count = len(lc_df[lc_df['Target'] == t])\n",
        "                rc_t_count = len(rc_df[rc_df['Target'] == t])\n",
        "                fraction_lc = lc_t_count/(lc_count + eps)\n",
        "                fraction_rc = rc_t_count/(rc_count + eps)\n",
        "                \n",
        "                if (fraction_lc > 0):\n",
        "                    lc_entropy -= fraction_lc * np.log2(fraction_lc)\n",
        "                elif (fraction_rc > 0):\n",
        "                    rc_entropy -= fraction_rc * np.log2(fraction_rc)\n",
        "                      \n",
        "            w_lc = lc_count/len(df)\n",
        "            w_rc = rc_count/len(df)\n",
        "            lc_entropy = w_lc * lc_entropy\n",
        "            rc_entropy = w_rc * rc_entropy\n",
        "            \n",
        "            if (lc_entropy + rc_entropy) < 1:\n",
        "                feature_entropy = lc_entropy + rc_entropy\n",
        "            else:\n",
        "                feature_entropy = 1\n",
        "                \n",
        "            #print(feature_entropy)\n",
        "                \n",
        "        return feature_entropy, threshold\n",
        "\n",
        "##############################################################################\n",
        "    \n",
        "    def predict(self, X):\n",
        "        results = []\n",
        "        #feature_search = {}\n",
        "        #print(X.index)\n",
        "        #col_index = 0\n",
        "        # for column in X.columns:\n",
        "        #     feature_search[column] = col_index\n",
        "        #     col_index += 1\n",
        "\n",
        "        for i in X.index:\n",
        "            \n",
        "            #print(X.loc[i])\n",
        "            row_prediction = self._predict_target_value(X.loc[i], self.tree)\n",
        "            results.append([i, row_prediction])\n",
        "        \n",
        "        results = pd.Series(results)\n",
        "        #print(results)\n",
        "        return results\n",
        "        \n",
        "                    \n",
        "###############################################################################\n",
        "    \n",
        "    def _predict_target_value(self, row, tree):\n",
        "        try:\n",
        "\n",
        "            for node in tree.keys():\n",
        "                result = row[node]\n",
        "                \n",
        "                if type(result) == str:\n",
        "                    #print(result)\n",
        "                    leaf = tree[node][result]\n",
        "                else:\n",
        "                    #print(str(list(tree[node].keys())[0]))\n",
        "                    \n",
        "                        \n",
        "                    try:\n",
        "                        threshold = str(list(tree[node].keys())[0]).split('<=')[1]\n",
        "                        \n",
        "                    except:\n",
        "                        threshold = str(list(tree[node].keys())[0]).split('>')[1]\n",
        "                        \n",
        "                    \n",
        "                    # if threshold == None:\n",
        "                    #     break\n",
        "                    \n",
        "                    #print('t is', threshold)\n",
        "                    if result <= float(threshold):\n",
        "                        leaf = tree[node]['<=' + str(threshold)]\n",
        "                        \n",
        "                    elif result > float(threshold):\n",
        "                        leaf = tree[node]['>' + str(threshold)]\n",
        "                        \n",
        "                if type(leaf) is dict:\n",
        "                    prediction = self._predict_target_value(row, leaf)\n",
        "                    \n",
        "                else:\n",
        "                    prediction = leaf\n",
        "                    \n",
        "            return prediction\n",
        "\n",
        "        except(KeyError):\n",
        "          print('keyerror encountered, returned random target value')\n",
        "          prediction = random.choice(['acc', 'unacc', 'good', 'vgood'])\n",
        "          return prediction\n",
        "###############################################################################\n",
        "def calc_accuracy(y_actual, y_pred):\n",
        "    #print(y_actual)\n",
        "    \n",
        "    #acc = number of correct predictions / number of predictions\n",
        "    \n",
        "    count_correct = 0\n",
        "    for i in range(len(y_actual)):\n",
        "        actual_target = y_actual.iloc[i]\n",
        "        predicted_target = y_pred.iloc[i][1]\n",
        "        \n",
        "        if predicted_target == actual_target:\n",
        "                count_correct += 1\n",
        "\n",
        "    \n",
        "    acc = round((count_correct/len(y_actual) * 100),2)\n",
        "    \n",
        "    return acc\n",
        "    \n",
        "############################################################################### \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    print('\\n-------------- Dataset------------------------')\n",
        "    \n",
        "    data = pd.read_csv('gdrive/My Drive/car.data')\n",
        "    \n",
        "    # should any column be full of only unique values, drop it\n",
        "    header = data.columns.values.tolist()\n",
        "\n",
        "    for column in header:\n",
        "      if len(data[column].unique()) == len(data):\n",
        "        data = data.drop([column], axis = 1)\n",
        "      \n",
        "    \n",
        "    data['buying'].value_counts()\n",
        "    for i in range(len(data)):\n",
        "        if data['buying'].iloc[i] == 'vhigh':\n",
        "            data['buying'].iloc[i] = 3\n",
        "        if data['buying'].iloc[i] == 'high':\n",
        "            data['buying'].iloc[i] = 2\n",
        "        if data['buying'].iloc[i] == 'med':\n",
        "            data['buying'].iloc[i] = 1\n",
        "        if data['buying'].iloc[i] == 'low':\n",
        "            data['buying'].iloc[i] = 0\n",
        "    data['buying'] = data['buying'].astype(int)\n",
        "    \n",
        "    data['maint'].value_counts()\n",
        "    for i in range(len(data)):\n",
        "        if data['maint'].iloc[i] == 'vhigh':\n",
        "            data['maint'].iloc[i] = 3\n",
        "        if data['maint'].iloc[i] == 'high':\n",
        "            data['maint'].iloc[i] = 2\n",
        "        if data['maint'].iloc[i] == 'med':\n",
        "            data['maint'].iloc[i] = 1\n",
        "        if data['maint'].iloc[i] == 'low':\n",
        "            data['maint'].iloc[i] = 0\n",
        "    data['maint'] = data['maint'].astype(int)\n",
        "        \n",
        "    data['doors'].value_counts()\n",
        "    for i in range(len(data)):\n",
        "        if data['doors'].iloc[i] == '5more':\n",
        "            data['doors'].iloc[i] = 3\n",
        "        if data['doors'].iloc[i] == '4':\n",
        "            data['doors'].iloc[i] = 2\n",
        "        if data['doors'].iloc[i] == '3':\n",
        "            data['doors'].iloc[i] = 1\n",
        "        if data['doors'].iloc[i] == '2':\n",
        "            data['doors'].iloc[i] = 0\n",
        "    data['doors'] = data['doors'].astype(int)\n",
        "        \n",
        "    data['persons'].value_counts()\n",
        "    for i in range(len(data)):\n",
        "        if data['persons'].iloc[i] == 'more':\n",
        "            data['persons'].iloc[i] = 2\n",
        "        if data['persons'].iloc[i] == '4':\n",
        "            data['persons'].iloc[i] = 1\n",
        "        if data['persons'].iloc[i] == '2':\n",
        "            data['persons'].iloc[i] = 0  \n",
        "    data['persons'] = data['persons'].astype(int)\n",
        "        \n",
        "    data['lug_boot'].value_counts()\n",
        "    for i in range(len(data)):\n",
        "        if data['lug_boot'].iloc[i] == 'big':\n",
        "            data['lug_boot'].iloc[i] = 2\n",
        "        if data['lug_boot'].iloc[i] == 'med':\n",
        "            data['lug_boot'].iloc[i] = 1\n",
        "        if data['lug_boot'].iloc[i] == 'small':\n",
        "            data['lug_boot'].iloc[i] = 0  \n",
        "    data['lug_boot'] = data['lug_boot'].astype(int)\n",
        "        \n",
        "    data['safety'].value_counts()\n",
        "    for i in range(len(data)):\n",
        "        if data['safety'].iloc[i] == 'high':\n",
        "            data['safety'].iloc[i] = 2\n",
        "        if data['safety'].iloc[i] == 'med':\n",
        "            data['safety'].iloc[i] = 1\n",
        "        if data['safety'].iloc[i] == 'low':\n",
        "            data['safety'].iloc[i] = 0 \n",
        "    data['safety'] = data['safety'].astype(int)\n",
        "            \n",
        "   \t#Split Features and target\n",
        "    X, y = data.drop(['car'], axis=1), data['car']\n",
        "    # print(X.head(), y.head())\n",
        "    X.describe()\n",
        "\n",
        "    ###########################################################################\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
        "    # dt_clf = DTClassifier(max_depth = 5)\n",
        "        \n",
        "    # dt_clf.fit(X_train, y_train)\n",
        "     \n",
        "    # prediction_results = dt_clf.predict(X_train)\n",
        "    # accuracy = calc_accuracy((y_train), prediction_results)\n",
        "    # print('\\nTrain Accuracy: %s' % accuracy)\n",
        "    \n",
        "    # accuracy = calc_accuracy((y_test), prediction_results)        \n",
        "    # print('Test Accuracy: %s' % (accuracy))\n",
        "    #Search for best depth\n",
        "    \n",
        "    y.value_counts()\n",
        "    trainAcc = []\n",
        "    testAcc = []\n",
        "\n",
        "    for d in range(1, 15, 5):\n",
        "        d_train_res = []\n",
        "        d_test_res = []\n",
        "        \n",
        "        for t in range(5):\n",
        "        \n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
        "        \n",
        "            dt_clf = DTClassifier(max_depth = d)\n",
        "        \n",
        "            dt_clf.fit(X_train, y_train)\n",
        "        \n",
        "                  ##############################################\n",
        "        \n",
        "            prediction_results = dt_clf.predict(X_train)\n",
        "        \n",
        "            accuracy = calc_accuracy((y_train), prediction_results)\n",
        "            d_train_res.append(accuracy)\n",
        "        \n",
        "            #print('\\n d=%s, Train Accuracy: %s' % (d, accuracy))\n",
        "        \n",
        "            prediction_results = dt_clf.predict(X_test)\n",
        "        \n",
        "            accuracy = calc_accuracy((y_test), prediction_results)\n",
        "            d_test_res.append(accuracy)\n",
        "        \n",
        "            #print('Test Accuracy: %s' % (accuracy))\n",
        "            \n",
        "        avg_d_train = round(mean(d_train_res), 2)\n",
        "        avg_d_test = round(mean(d_test_res), 2)\n",
        "        \n",
        "        trainAcc.append(avg_d_train)\n",
        "        testAcc.append(avg_d_test)\n",
        "    print('\\n')\n",
        "    print('Average training accuracy from 5 iterations by increasing depth left to right\\n', trainAcc)\n",
        "    print('Average testing accuracy from 5 iterations by increasing depth left to right\\n', testAcc)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-------------- Dataset------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Average training accuracy from 5 iterations by increasing depth left to right\n",
            " [70.27, 70.15, 70.8]\n",
            "Average testing accuracy from 5 iterations by increasing depth left to right\n",
            " [69.02, 69.42, 70.98]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBvUMJf7HPLB"
      },
      "source": [
        "_______________________________________________________________________________\n",
        "Conclusion\n",
        "\n",
        "This was an extremely challenging assignment for me, to which I invested way more time than I anticipated.  In doing so however, I learned way more about decision trees than expected and have a profound appreciation for the power of a robustly created algorithm that can handled data of all types across large feature sets.  I understand the theory of the decision tree, how to calculate and select the feature with the highest information gain and how this reduces impurity.  My frustrations and shortcomings with implementation largely stem from construction the \"physical structure\" that is the tree.  \n",
        "\n",
        "Thank you for your time.\n",
        "\n",
        "- Justin"
      ]
    }
  ]
}